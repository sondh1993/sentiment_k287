import warnings
warnings.filterwarnings("ignore")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
plt.style.use('ggplot')
import underthesea
from underthesea import word_tokenize, pos_tag, sent_tokenize
import re
import streamlit as st

st.set_page_config(page_title="Sentiment Analysis", page_icon="ğŸ“ˆ")
data = pd.read_csv('Sendo_reviews.csv')
# Upload file
uploaded_file = st.file_uploader("Choose a file", type=['csv'])
if uploaded_file is not None:
    data = pd.read_csv(uploaded_file, encoding='latin-1')
    data.to_csv("spam_new.csv", index = False)

# Äá»c file trÃ­ch xuáº¥t
def file_to_dict(path):
    file = open(path, 'r', encoding = 'utf-8')
    lst = file.read().split('\n')
    dict = {}
    for line in lst:
        key, value = line.split('\t')
        dict[key] = str(value)
    return dict
def file_to_list(path):
    file = open(path, 'r', encoding = 'utf-8')
    lst = file.read().split('\n')
    return lst

################################
def find_words(document, list_of_words):
    document_lower = document.lower()
    word_count = 0
    word_list = []

    for word in list_of_words:
        if word in document_lower:
            print(word)
            word_count += document_lower.count(word)
            word_list.append(word)

    return word_count, word_list

def count_value_text(row, find_text):
    row = row.lower()
    count = sum(1 for text in find_text if text in row)
    return count


def pre_text(text, emoji_dict, teen_dict, wrong_lst):
    doc = text.lower()
    doc = doc.replace("'",'')
    doc = re.sub(r'\.+', ".",doc)
    new_sentiment = ''
    for sentence in sent_tokenize(doc):
        # Cover emojicon
        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))
        # conver teencode
        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())
        # dell 
        pattern = r'(?i)\b[b-zÃ¡Ã¢Ã Äƒáº¯áº±áºµáº·áº¥áº§áº«áº©áº­bcdÄ‘eÃ©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡fghÃ­Ã¬á»‰Ä©á»‹jklmnoÃ³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£pqrstuÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±vwxyÃ½á»³á»·á»¹á»µz]+\b'
        sentence = ' '.join(re.findall(pattern, sentence))
        # del wrong words
        sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())
        new_sentiment = new_sentiment+ sentence + '. '
    doc = new_sentiment
    doc = re.sub(r'\s+', ' ', doc).strip()
    return doc
# Chuáº©n hÃ³a unicode tiáº¿ng viá»‡t
def loaddicchar(txt):
    uniChars = "Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"

    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
        '|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return re.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dicchar[x.group()], txt)
def process_special_word(text):
    # cÃ³ thá»ƒ cÃ³ nhiá»u tá»« Ä‘áº·c biá»‡t cáº§n rÃ¡p láº¡i vá»›i nhau
    new_text = ''
    text_lst = text.split()
    i= 0
    # khÃ´ng, cháº³ng, cháº£...
    if 'khÃ´ng' in text_lst:
        while i <= len(text_lst) - 1:
            word = text_lst[i]
            #print(word)
            #print(i)
            if  word == 'khÃ´ng':
                next_idx = i+1
                if next_idx <= len(text_lst) -1:
                    word = word +'_'+ text_lst[next_idx]
                i= next_idx + 1
            else:
                i = i+1
            new_text = new_text + word + ' '
    else:
        new_text = text
    return new_text.strip()
################################
def process_postag_thesea(text):
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.','')
        ###### POS tag
        lst_word_type = ['N','Np','A','AB','V','VB','VY','R']
        # lst_word_type = ['A','AB','V','VB','VY','R']
        sentence = ' '.join( word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format="text"))))
        new_document = new_document + sentence + ' '
    ###### DEL excess blank space
    new_document = re.sub(r'\s+', ' ', new_document).strip()
    return new_document
################################
def remove_stop(text, stopwords):
    doc = ' '.join('' if word in stopwords else word for word in text.split())
    doc = re.sub(r'\s+', ' ', doc).strip()
    return doc
################################
def find_words(document, list_of_words):
    document_lower = document.lower()
    word_count = 0
    word_list = []

    for word in list_of_words:
        if word in document_lower:
            print(word)
            word_count += document_lower.count(word)
            word_list.append(word)

    return word_count, word_list
def count_value_text(row, find_text):
    row = row.lower()
    count = sum(1 for text in find_text if text in row)
    return count

# file cÃ³ sáºµn clean text
emoji_dict = file_to_dict(r'emojicon.txt')
teen_dict = file_to_dict(r'teencode.txt')
envi_dict = file_to_dict(r'english-vnmese.txt')
wrong_lst = file_to_list(r'wrong-word.txt')
stop_lst = file_to_list(r'vietnamese-stopwords.txt')
# load file -> create new columns from the values enumeratedsentences
positive_emoji_dict = file_to_list(r'positive_emoji.txt')
negative_emoji_dict = file_to_list(r'negative_emoji.txt')

positive_words_dict = file_to_list(r'positive_words.txt')
negative_words_dict = file_to_list(r'negative_words.txt')

def step_2(df_sub):
    df_sub['words'] = df_sub['content'].apply(lambda x: pre_text(str(x), emoji_dict, teen_dict, wrong_lst))
    df_sub['words'] = df_sub['words'].apply(lambda x: loaddicchar(str(x)))
    df_sub['words'] = df_sub['words'].apply(lambda x: process_postag_thesea(str(x)))
    df_sub['words'] = df_sub['words'].apply(lambda x: remove_stop(str(x), stop_lst))
    df_sub['positive_words_count'] = df_sub['words'].apply(lambda x: count_value_text(str(x), positive_words_dict))
    df_sub['positive_emoji_count'] = df_sub['words'].apply(lambda x: count_value_text(str(x), positive_emoji_dict))
    df_sub['negative_words_count'] = df_sub['words'].apply(lambda x: count_value_text(str(x), negative_words_dict))
    df_sub['negative_emoji_count'] = df_sub['words'].apply(lambda x: count_value_text(str(x), negative_emoji_dict))
    df_sub['words_length'] = df_sub['words'].apply(lambda x: len(str(x).split()))
    # ThÃªm 2 cá»™t sentiment vÃ  Ä‘Ã¡nh giÃ¡ láº¡i rating
    df_sub['positive'] = df_sub['positive_emoji_count'] + df_sub['positive_words_count']
    df_sub['negative'] = df_sub['negative_emoji_count'] + df_sub['negative_words_count']
    # thÃªm cá»™t má»›i rating_new
    df_sub['rating_new'] = np.where(df_sub['positive'] > df_sub['negative'], df_sub['rating']+1, df_sub['rating'] -1 )
    # Táº¡o cá»™t má»›i sentiment káº¿t hÆ¡p tá»« rating 
    df_sub['sentiment'] = df_sub['rating_new'].apply(lambda x: '2' if x >= 4 else '0' if x <= 2 else '1')
    # XÃ³a dá»¯ liá»‡u thiáº¿u
    df_sub = df_sub.dropna()
    df_sub = df_sub.drop(['content','rating'], axis=1)
    return df_sub



# GUI
menu = ["Business Objective", "EDA", "Build Project", "New Prediction"]
choice = st.sidebar.selectbox('Menu', menu)
if choice =="Business Objective":
    st.title("Data Project 3")
    st.write(""" XÃ¢y dá»±ng mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n:  
    - giÃºp ngÆ°á»i bÃ¡n hÃ ng cÃ³ thá»ƒ biáº¿t Ä‘Æ°á»£c nhá»¯ng pháº£n há»“i nhanh chÃ³ng cá»§a khÃ¡ch hÃ ng vá» sáº£n pháº©m hay dá»‹ch vá»¥ cá»§a há» (tÃ­ch cá»±c, tiÃªu cá»±c hay trung tÃ­nh), 
    - Ä‘iá»u nÃ y giÃºp cho ngÆ°á»i bÃ¡n biáº¿t Ä‘Æ°á»£c tÃ¬nh hÃ¬nh kinh doanh, hiá»ƒu Ä‘Æ°á»£c Ã½ kiáº¿n cá»§a khÃ¡ch hÃ ng tá»« Ä‘Ã³ giÃºp há» cáº£i thiá»‡n hÆ¡n trong dá»‹ch vá»¥, sáº£n pháº©m """)
    st.markdown("Dá»¯ Liá»‡u :")
    
    st.dataframe(data.head())
    
elif choice== "EDA":
    st.header("Exploratory Data Analysis")
    st.dataframe(data.sample(20))
    # cá»™t content cáº§n Ä‘Æ°á»£c xá»­ lÃ½
    df_sub = data[['content','rating']].copy()
    df_sub= step_2(df_sub)
    st.markdown("Biá»ƒu Ä‘á»“ Ä‘áº¿m sá»‘ lÆ°á»£ng sentiment")
    # Táº¡o biá»ƒu Ä‘á»“ Ä‘áº¿m sá»‘ lÆ°á»£ng cÃ¡c giÃ¡ trá»‹ trong cá»™t "sentiment"
    ax = sns.countplot(x=df_sub['sentiment'], order=df_sub['sentiment'].value_counts(ascending=True).index)

    # ThÃªm nhÃ£n sá»‘ lÆ°á»£ng lÃªn trÃªn cÃ¡c cá»™t
    abs_value = df_sub['sentiment'].value_counts(ascending=True).values
    ax.bar_label(ax.containers[0], labels=abs_value)

    # Hiá»ƒn thá»‹ biá»ƒu Ä‘á»“
    st.pyplot(plt)
